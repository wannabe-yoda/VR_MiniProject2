{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11748083,"sourceType":"datasetVersion","datasetId":7375098},{"sourceId":11748127,"sourceType":"datasetVersion","datasetId":7375132},{"sourceId":390363,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321527,"modelId":342134}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"Imports go here\"\"\"\n\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\nimport torch\nfrom PIL import Image\nimport requests\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n#from word2number import w2n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:49:00.625202Z","iopub.execute_input":"2025-05-13T12:49:00.625496Z","iopub.status.idle":"2025-05-13T12:49:00.643086Z","shell.execute_reply.started":"2025-05-13T12:49:00.625474Z","shell.execute_reply":"2025-05-13T12:49:00.642510Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\"\"\" Load CSV and split the data \"\"\"\n\n# Load the CSV file\ncsv_path = \"/kaggle/input/qna-final/qna_final.csv\"\ndf = pd.read_csv(csv_path)\n\n# Get unique Item_IDs\nunique_ids = df[\"Item_ID\"].unique()\n\n# Set random seed for reproducibility\nrandom_seed = 42\nnp.random.seed(random_seed)\n\n# Shuffle and split the unique IDs\ntrain_ids, temp_ids = train_test_split(unique_ids, test_size=0.3, random_state=random_seed)  # 70% train\nval_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=random_seed)      # 15% val, 15% test\n\n# Create train, val, and test DataFrames\ntrain_df = df[df[\"Item_ID\"].isin(train_ids)]\nval_df = df[df[\"Item_ID\"].isin(val_ids)]\ntest_df = df[df[\"Item_ID\"].isin(test_ids)]\n\nprint(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:49:12.285756Z","iopub.execute_input":"2025-05-13T12:49:12.286307Z","iopub.status.idle":"2025-05-13T12:49:12.332569Z","shell.execute_reply.started":"2025-05-13T12:49:12.286283Z","shell.execute_reply":"2025-05-13T12:49:12.331984Z"}},"outputs":[{"name":"stdout","text":"Train size: 16666, Val size: 3663, Test size: 3558\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\"\"\"Normalizing and adding non-existing answers to label2id (with test leakage prevention)\"\"\"\n# Load the model and processor\nmodel_config_source = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n# Text-to-number mapping (same as before)\ndef get_text_to_num_mapping():\n    text_to_num = {\n        \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n        \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\",\n        \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\", \"thirteen\": \"13\",\n        \"fourteen\": \"14\", \"fifteen\": \"15\", \"sixteen\": \"16\", \"seventeen\": \"17\",\n        \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\",\n    }\n    for i in range(21, 1001):\n        text_to_num[str(i)] = str(i)\n    return text_to_num\n\ntext_to_num_map = get_text_to_num_mapping()\n\ndef normalize_answer(answer_str):\n    normalized = str(answer_str).strip().lower()\n    return text_to_num_map.get(normalized, normalized)\n\n# Create DataFrames with .copy()\ntrain_df = df[df[\"Item_ID\"].isin(train_ids)].copy()\nval_df = df[df[\"Item_ID\"].isin(val_ids)].copy()\ntest_df = df[df[\"Item_ID\"].isin(test_ids)].copy()\n\n# Add normalized answers\ntrain_df['normalized_answer'] = train_df['Answer'].apply(normalize_answer)\nval_df['normalized_answer'] = val_df['Answer'].apply(normalize_answer)\ntest_df['normalized_answer'] = test_df['Answer'].apply(normalize_answer)\n\n# --- KEY CHANGE: Only use TRAINING answers to extend label2id ---\nall_train_answers = set(train_df['normalized_answer'].unique())\noriginal_label2id = model_config_source.config.label2id\n\n# Extend label2id with NEW TRAINING ANSWERS only\nnew_answers = [ans for ans in all_train_answers if ans not in original_label2id]\ncurrent_max_id = max(original_label2id.values())\n\nextended_label2id = {**original_label2id}\nextended_id2label = {**model_config_source.config.id2label}\n\nfor idx, ans in enumerate(new_answers, start=current_max_id + 1):\n    extended_label2id[ans] = idx\n    extended_id2label[idx] = ans\n\n# --- Filter validation/test sets to keep only answers in extended_label2id ---\ndef filter_to_valid_labels(df, label2id):\n    \"\"\"Remove rows with answers not in label2id\"\"\"\n    valid_mask = df['normalized_answer'].isin(label2id)\n    invalid_count = len(df) - valid_mask.sum()\n    if invalid_count > 0:\n        print(f\"Filtered {invalid_count} rows with unseen answers\")\n    return df[valid_mask].copy()\n\nval_df_filtered = filter_to_valid_labels(val_df, extended_label2id)\ntest_df_filtered = filter_to_valid_labels(test_df, extended_label2id)\n\n# --- Final Check ---\nprint(\"\\nFinal sizes:\")\nprint(f\"Train: {len(train_df)}\")\nprint(f\"Val (filtered): {len(val_df_filtered)}\")\nprint(f\"Test (filtered): {len(test_df_filtered)}\")\nprint(f\"Total labels: {len(extended_label2id)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:49:15.606370Z","iopub.execute_input":"2025-05-13T12:49:15.607079Z","iopub.status.idle":"2025-05-13T12:49:16.316164Z","shell.execute_reply.started":"2025-05-13T12:49:15.607041Z","shell.execute_reply":"2025-05-13T12:49:16.315425Z"}},"outputs":[{"name":"stdout","text":"Filtered 161 rows with unseen answers\nFiltered 236 rows with unseen answers\n\nFinal sizes:\nTrain: 16666\nVal (filtered): 3502\nTest (filtered): 3322\nTotal labels: 3464\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\"\"\"Create a custom dataset class\"\"\"\n\n#text_form = w2n.to_words(i)  # Convert integer to text form\n\nclass QnADataset(Dataset):\n    def __init__(self, dataframe, image_dir, processor, label2id): # Processor is not strictly needed here anymore, but label2id is\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        # self.processor = processor # Not used directly in __getitem__ anymore\n        self.label2id = label2id\n        self.text_to_num = self.generate_text_to_num_mapping()\n\n    def generate_text_to_num_mapping(self):\n        # (Your existing generate_text_to_num_mapping method - keep as is)\n        text_to_num = {\n            \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n            \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\",\n            \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\", \"thirteen\": \"13\",\n            \"fourteen\": \"14\", \"fifteen\": \"15\", \"sixteen\": \"16\", \"seventeen\": \"17\",\n            \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\",\n        }\n        for i in range(21, 1001):\n            text_to_num[str(i)] = str(i)\n        return text_to_num\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = f\"{self.image_dir}/{row['Image_Path']}\"\n        question_text = row[\"Question\"]  # Keep as raw text\n        answer_str = row[\"Answer\"].strip().lower()\n\n        # Convert text-based numbers to numerical strings if needed\n        if answer_str in self.text_to_num:\n            processed_answer_str = self.text_to_num[answer_str]\n        else:\n            processed_answer_str = answer_str\n\n        # Load PIL image\n        try:\n            pil_image = Image.open(image_path).convert(\"RGB\")\n        except FileNotFoundError:\n            print(f\"Error: Image not found at {image_path}\")\n            # Handle appropriately: skip, return None, or use a placeholder\n            # For now, let's re-raise to make it obvious during debugging\n            raise\n        except Exception as e:\n            print(f\"Error loading image {image_path}: {e}\")\n            raise\n\n\n        # Encode the answer string to an ID\n        if processed_answer_str in self.label2id:\n            answer_id = self.label2id[processed_answer_str]\n        else:\n            # This can be a common issue. Ensure your label2id covers all possible answers\n            # or you have a strategy for unknown answers.\n            print(f\"Warning: Answer '{processed_answer_str}' (original: '{row['Answer']}') not found in label2id mapping. Item index: {idx}, Image: {row['Image_Path']}\")\n            # Option 1: Raise error (as you had)\n            # raise ValueError(f\"Answer '{processed_answer_str}' not found in label2id mapping.\")\n            # Option 2: Assign a default/unknown ID if you have one, or skip sample.\n            # For now, let's make it return something that might cause a downstream error if not handled,\n            # or you can choose to filter these out in collate_fn or raise error.\n            # To proceed, we'll assume for now this case should be an error or filtered.\n            # For robust training, you'd need a clear strategy.\n            raise ValueError(f\"Answer '{processed_answer_str}' (from original '{row['Answer']}') not found in label2id mapping for image {row['Image_Path']}.\")\n\n\n        return {\n            \"image\": pil_image,          # Return the PIL Image object\n            \"question\": question_text,   # Return the raw question string\n            \"labels\": torch.tensor(answer_id, dtype=torch.long) # Return the label as a tensor\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:49:20.267251Z","iopub.execute_input":"2025-05-13T12:49:20.267869Z","iopub.status.idle":"2025-05-13T12:49:20.276797Z","shell.execute_reply.started":"2025-05-13T12:49:20.267844Z","shell.execute_reply":"2025-05-13T12:49:20.275973Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\"\"\" Prepare dataloaders \"\"\"\nfrom functools import partial\n\n# Use extended_label2id instead of reloading the model\nnum_labels = len(extended_label2id)\n\n# Directory containing the images\nimage_dir = \"/kaggle/input/filtered-small-amazon-qna\"\n\n# Create datasets with EXTENDED labels\ntrain_dataset = QnADataset(train_df, image_dir, processor, extended_label2id)\nval_dataset = QnADataset(val_df_filtered, image_dir, processor, extended_label2id)\ntest_dataset = QnADataset(test_df_filtered, image_dir, processor, extended_label2id)\n\n# Corrected collate function with proper argument order\ndef collate_fn(batch, processor, num_classes=num_labels):\n    \"\"\"ViLT-compatible collate function with one-hot encoding\"\"\"\n    # Filter out invalid entries\n    valid_batch = [\n        item for item in batch \n        if item is not None \n        and isinstance(item.get(\"image\"), Image.Image)\n        and item.get(\"question\") \n        and item.get(\"labels\") is not None\n    ]\n    \n    if not valid_batch:\n        return None\n    \n    # Process valid items\n    images = [item[\"image\"] for item in valid_batch]\n    texts = [item[\"question\"] for item in valid_batch]\n    labels = [item[\"labels\"] for item in valid_batch]  # Should be class indices\n\n    # Process through processor\n    try:\n        encoding = processor(\n            images=images,\n            text=texts,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            truncation=True,\n            max_length=512\n        )\n    except Exception as e:\n        print(f\"Skipping batch: {str(e)}\")\n        return None\n\n    # Convert labels to one-hot encoding\n    batch_size = len(labels)\n    one_hot_labels = torch.zeros(batch_size, num_classes)\n    for i, label in enumerate(labels):\n        one_hot_labels[i, label] = 1.0\n\n    encoding[\"labels\"] = one_hot_labels\n    return encoding\n\n# Create DataLoaders with proper partial binding\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=True,\n    collate_fn=partial(collate_fn, processor=processor),  # Keyword argument binding\n    drop_last=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=partial(collate_fn, processor=processor)  # Keyword argument binding\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=partial(collate_fn, processor=processor)  # Keyword argument binding\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:49:24.283452Z","iopub.execute_input":"2025-05-13T12:49:24.284117Z","iopub.status.idle":"2025-05-13T12:49:24.293042Z","shell.execute_reply.started":"2025-05-13T12:49:24.284092Z","shell.execute_reply":"2025-05-13T12:49:24.292254Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Fine tuning part","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport os\nimport numpy as np\nfrom peft import LoraConfig, get_peft_model\n\n# --- Configuration ---\nNUM_EPOCHS = 20\nLEARNING_RATE = 1e-4\nOUTPUT_DIR = \"/kaggle/working/vilt-lora-manual-best\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- Model Setup ---\noriginal_model = ViltForQuestionAnswering.from_pretrained(\n    \"dandelin/vilt-b32-finetuned-vqa\",\n    num_labels=len(extended_label2id),\n    id2label=extended_id2label,\n    label2id=extended_label2id,\n    ignore_mismatched_sizes=True\n)\n\n# --- PEFT Configuration ---\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"classifier\"]\n)\npeft_model = get_peft_model(original_model, lora_config)\npeft_model.print_trainable_parameters()\npeft_model.to(device)\n\n# --- Optimizer ---\noptimizer = optim.AdamW(peft_model.parameters(), lr=LEARNING_RATE)\n\nprint(f\"\\n--- Starting Manual Training for {NUM_EPOCHS} epochs ---\")\nbest_val_loss = float('inf')\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n\n    # --- Training Phase ---\n    peft_model.train()\n    total_train_loss, train_batches = 0, 0\n    \n    progress_bar_train = tqdm(train_loader, desc=\"Training\", leave=False)\n    for batch in progress_bar_train:\n        # Skip invalid/empty batches\n        if batch is None:\n            print(\"Skipping empty training batch\")\n            continue\n\n        try:\n            # Move batch to device\n            batch_on_device = {k: v.to(device) for k, v in batch.items()}\n            \n            # Forward pass\n            optimizer.zero_grad()\n            outputs = peft_model(**batch_on_device)\n            loss = outputs.loss\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            # Update metrics\n            total_train_loss += loss.item()\n            train_batches += 1\n            progress_bar_train.set_postfix(batch_loss=loss.item())\n\n        except Exception as e:\n            print(f\"\\nError in training batch: {str(e)}\")\n            print(\"Skipping problematic training batch\")\n            continue\n\n    # --- Validation Phase ---\n    peft_model.eval()\n    total_val_loss, val_batches = 0, 0\n    \n    progress_bar_val = tqdm(val_loader, desc=\"Validation\", leave=False)\n    with torch.no_grad():\n        for batch in progress_bar_val:\n            # Skip invalid/empty batches\n            if batch is None:\n                print(\"Skipping empty validation batch\")\n                continue\n\n            try:\n                # Move batch to device\n                batch_on_device = {k: v.to(device) for k, v in batch.items()}\n                \n                # Forward pass\n                outputs = peft_model(**batch_on_device)\n                loss = outputs.loss\n\n                # Update metrics\n                total_val_loss += loss.item()\n                val_batches += 1\n                progress_bar_val.set_postfix(batch_loss=loss.item())\n\n            except Exception as e:\n                print(f\"\\nError in validation batch: {str(e)}\")\n                print(\"Skipping problematic validation batch\")\n                continue\n\n    # Calculate epoch metrics\n    avg_train_loss = total_train_loss / train_batches if train_batches > 0 else float('inf')\n    avg_val_loss = total_val_loss / val_batches if val_batches > 0 else float('inf')\n    \n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n    # Save best model\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        print(f\"New best model! Saving to {OUTPUT_DIR}\")\n        peft_model.save_pretrained(OUTPUT_DIR)\n\nprint(\"\\n--- Training Finished ---\")\nprint(f\"Best Validation Loss: {best_val_loss:.4f}\")\nprint(f\"Model saved to: {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T03:55:56.746007Z","iopub.execute_input":"2025-05-11T03:55:56.746219Z","iopub.status.idle":"2025-05-11T03:56:57.097557Z","shell.execute_reply.started":"2025-05-11T03:55:56.746195Z","shell.execute_reply":"2025-05-11T03:56:57.092909Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-finetuned-vqa and are newly initialized because the shapes did not match:\n- classifier.3.weight: found shape torch.Size([3129, 1536]) in the checkpoint and torch.Size([3464, 1536]) in the model instantiated\n- classifier.3.bias: found shape torch.Size([3129]) in the checkpoint and torch.Size([3464]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 6,803,336 || all params: 124,906,768 || trainable%: 5.4467\n\n--- Starting Manual Training for 20 epochs ---\n\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1241383957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Update metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mprogress_bar_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"!pip install bert-score\n!git clone https://github.com/neulab/BARTScore.git\nimport sys\nsys.path.append(\"./BARTScore\")\n# Now import\nfrom bart_score import BARTScorer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:45:46.497887Z","iopub.execute_input":"2025-05-13T12:45:46.498454Z","iopub.status.idle":"2025-05-13T12:47:10.552880Z","shell.execute_reply.started":"2025-05-13T12:45:46.498416Z","shell.execute_reply":"2025-05-13T12:47:10.552173Z"}},"outputs":[{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCloning into 'BARTScore'...\nremote: Enumerating objects: 220, done.\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 220 (delta 18), reused 14 (delta 14), pack-reused 194 (from 1)\u001b[K\nReceiving objects: 100% (220/220), 101.98 MiB | 23.10 MiB/s, done.\nResolving deltas: 100% (47/47), done.\nUpdating files: 100% (192/192), done.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import sys\nimport time\nimport torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# 1. Add local BARTScore code into Python’s import path\nsys.path.append(\"./BARTScore\")\n\n# 2. Semantic‐similarity imports\nfrom bert_score import score as bert_score\nfrom bart_score import BARTScorer\n\n# 3. PEFT & model imports\nfrom transformers import ViltForQuestionAnswering\nfrom peft import PeftModel\n\n# 4. Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# 5. Reload base + LoRA‐finetuned model\n#    (assumes you previously saved to OUTPUT_DIR)\nOUTPUT_DIR = \"/kaggle/input/lora_vilt_best/transformers/default/1/vilt-lora-manual-best\"\nbase_model = ViltForQuestionAnswering.from_pretrained(\n    \"dandelin/vilt-b32-finetuned-vqa\",\n    num_labels=len(extended_label2id),\n    id2label=extended_id2label,\n    label2id=extended_label2id,\n    ignore_mismatched_sizes=True\n)\nmodel = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\nmodel.to(device)\nmodel.eval()\n\n# 6. Accumulators\nall_pred_ids   = []\nall_true_ids   = []\nall_pred_texts = []\nall_true_texts = []\n\n# --- Start overall timer ---\nt0_overall = time.time()\n\n# 7. Inference + gather labels/texts with progress bar\nt0_loop = time.time()\nfor batch in tqdm(test_loader, desc=\"Evaluating batches\"):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    outputs = model(**batch)\n    logits  = outputs.logits\n\n    # Predicted & true IDs\n    pred_ids = logits.argmax(dim=-1)\n    true_ids = batch[\"labels\"].argmax(dim=-1)\n\n    # Flatten for metrics\n    pred_flat = pred_ids.view(-1).cpu().numpy()\n    true_flat = true_ids.view(-1).cpu().numpy()\n    all_pred_ids.extend(pred_flat)\n    all_true_ids.extend(true_flat)\n\n    # Convert to label strings\n    all_pred_texts.extend([model.config.id2label[i] for i in pred_flat])\n    all_true_texts.extend([model.config.id2label[i] for i in true_flat])\nt1_loop = time.time()\nprint(f\"\\nInference & gathering took {t1_loop - t0_loop:.2f}s\")\n\n# 8. Classification metrics\nt0_cls = time.time()\naccuracy  = accuracy_score(all_true_ids, all_pred_ids)\nprecision = precision_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\nrecall    = recall_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\nf1        = f1_score(all_true_ids, all_pred_ids, average=\"macro\", zero_division=0)\nt1_cls = time.time()\n\nprint(f\"\\nClassification metrics computed in {t1_cls - t0_cls:.2f}s\")\nprint(\"=== Classification Metrics ===\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"Precision (M) : {precision:.4f}\")\nprint(f\"Recall    (M) : {recall:.4f}\")\nprint(f\"F1 Score  (M) : {f1:.4f}\")\n\n# 9. BERTScore (semantic similarity)\nt0_bert = time.time()\nbert_p, bert_r, bert_f1 = bert_score(\n    all_pred_texts,\n    all_true_texts,\n    lang=\"en\",\n    model_type=\"bert-base-uncased\",\n    rescale_with_baseline=True\n)\nt1_bert = time.time()\nprint(f\"\\nBERTScore computed in {t1_bert - t0_bert:.2f}s\")\nprint(\"=== BERTScore ===\")\nprint(f\"Precision : {bert_p.mean().item():.4f}\")\nprint(f\"Recall    : {bert_r.mean().item():.4f}\")\nprint(f\"F1        : {bert_f1.mean().item():.4f}\")\n\n# 10. BARTScore (semantic entailment)\nt0_bart = time.time()\nbart_scorer = BARTScorer(device=device.type, checkpoint=\"facebook/bart-large-cnn\")\nbart_scores = bart_scorer.score(\n    all_pred_texts,\n    all_true_texts,\n    batch_size=8\n)\nt1_bart = time.time()\nmean_bart = sum(bart_scores) / len(bart_scores)\nprint(f\"\\nBARTScore computed in {t1_bart - t0_bart:.2f}s\")\nprint(\"=== BARTScore ===\")\nprint(f\"Mean score: {mean_bart:.4f}\")\n\n# --- End overall timer ---\nt1_overall = time.time()\nprint(f\"\\nTotal evaluation time: {t1_overall - t0_overall:.2f}s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:50:00.700513Z","iopub.execute_input":"2025-05-13T12:50:00.700814Z","iopub.status.idle":"2025-05-13T12:51:54.250743Z","shell.execute_reply.started":"2025-05-13T12:50:00.700791Z","shell.execute_reply":"2025-05-13T12:51:54.250040Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-finetuned-vqa and are newly initialized because the shapes did not match:\n- classifier.3.weight: found shape torch.Size([3129, 1536]) in the checkpoint and torch.Size([3464, 1536]) in the model instantiated\n- classifier.3.bias: found shape torch.Size([3129]) in the checkpoint and torch.Size([3464]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating batches: 100%|██████████| 416/416 [01:27<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"\nInference & gathering took 87.82s\n\nClassification metrics computed in 0.02s\n=== Classification Metrics ===\nAccuracy      : 0.6367\nPrecision (M) : 0.0604\nRecall    (M) : 0.0810\nF1 Score  (M) : 0.0643\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00fe38895ccd4f51b1ad3423fdb9021d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad2b5ed69a504825b074640b19d4d3ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a8a2aef1e614751a9eb24cd69feb766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531e237c9f434d7a8e93cbba1498e822"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eff8550b8264005b6b295ee0caf1c99"}},"metadata":{}},{"name":"stdout","text":"\nBERTScore computed in 7.08s\n=== BERTScore ===\nPrecision : 0.8009\nRecall    : 0.8006\nF1        : 0.7993\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80872e955f7d490abded45bd56f1cb8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfdd478631a54a4cb2d11cc19d1d764c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39d725365b5f4685a335edb2d1597d38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758f931bfd9d45188ec5834ad6b7bdf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62a42ccb3dea4eaab093300ac34270b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f833042bc94fa286a1b0187b847a85"}},"metadata":{}},{"name":"stdout","text":"\nBARTScore computed in 17.67s\n=== BARTScore ===\nMean score: -3.5814\n\nTotal evaluation time: 112.58s\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import shutil\nimport os\n\nfolder_to_zip = \"/kaggle/working/vilt-lora-manual-best\"\noutput_zip_name = \"/kaggle/working/vilt-lora-manual-best_model\" # Name for the zip file (no .zip here)\n\ntry:\n    shutil.make_archive(output_zip_name,  # The name of the file to create (e.g., /kaggle/working/my_model_archive)\n                        'zip',             # The format (zip, tar, etc.)\n                        root_dir=os.path.dirname(folder_to_zip), # The directory containing the folder to zip\n                        base_dir=os.path.basename(folder_to_zip)) # The folder to zip\n\n    print(f\"Successfully created zip file: {output_zip_name}.zip\")\n    print(f\"You can now find '{os.path.basename(output_zip_name)}.zip' in the Output section on the right sidebar (or under /kaggle/working/) and download it.\")\nexcept FileNotFoundError:\n    print(f\"Error: The folder {folder_to_zip} was not found. Please check the path.\")\nexcept Exception as e:\n    print(f\"An error occurred during zipping: {e}\")\n\n# Optional: Display a download link (might not always work perfectly in all browsers/setups)\nfrom IPython.display import FileLink, display\nif os.path.exists(output_zip_name + \".zip\"):\n    print(\"\\nAttempting to display a download link (click to download):\")\n    display(FileLink(output_zip_name + \".zip\"))\nelse:\n    print(f\"\\nZip file {output_zip_name}.zip not found for creating a direct link. Please check the Output section manually.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T03:58:02.674897Z","iopub.execute_input":"2025-05-11T03:58:02.675617Z","iopub.status.idle":"2025-05-11T03:58:04.008316Z","shell.execute_reply.started":"2025-05-11T03:58:02.675595Z","shell.execute_reply":"2025-05-11T03:58:04.007516Z"}},"outputs":[{"name":"stdout","text":"Successfully created zip file: /kaggle/working/vilt-lora-manual-best_model.zip\nYou can now find 'vilt-lora-manual-best_model.zip' in the Output section on the right sidebar (or under /kaggle/working/) and download it.\n\nAttempting to display a download link (click to download):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/vilt-lora-manual-best_model.zip","text/html":"<a href='/kaggle/working/vilt-lora-manual-best_model.zip' target='_blank'>/kaggle/working/vilt-lora-manual-best_model.zip</a><br>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Define the output zip file name\nzip_filename = '/kaggle/working/output.zip'\n\n# List of files and directories to include\nitems_to_zip = [\n    'vilt-lora-manual-best',\n    'README.md',\n    'adapter_config.json',\n    'adapter_model.safetensors'\n]\n\ndef zip_directory(path, ziph):\n    # Walk through all files in the directory\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Add file to zip file, preserving directory structure\n            ziph.write(file_path, os.path.relpath(file_path, os.path.dirname(path)))\n\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for item in items_to_zip:\n        if os.path.isdir(item):\n            # Add directory recursively\n            zip_directory(item, zipf)\n        elif os.path.isfile(item):\n            # Add single file\n            zipf.write(item)\n            \nprint(f\"Zip file created at: {zip_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T04:06:39.277565Z","iopub.execute_input":"2025-05-11T04:06:39.278121Z","iopub.status.idle":"2025-05-11T04:06:40.689480Z","shell.execute_reply.started":"2025-05-11T04:06:39.278095Z","shell.execute_reply":"2025-05-11T04:06:40.688607Z"}},"outputs":[{"name":"stdout","text":"Zip file created at: /kaggle/working/output.zip\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!ls -lh /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T04:05:59.955774Z","iopub.execute_input":"2025-05-11T04:05:59.956502Z","iopub.status.idle":"2025-05-11T04:06:00.113549Z","shell.execute_reply.started":"2025-05-11T04:05:59.956480Z","shell.execute_reply":"2025-05-11T04:06:00.112505Z"}},"outputs":[{"name":"stdout","text":"total 25M\ndrwxr-xr-x 2 root root 4.0K May 10 18:08 vilt-lora-manual-best\n-rw-r--r-- 1 root root  25M May 11 03:58 vilt-lora-manual-best_model.zip\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"exit()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T04:08:54.653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
