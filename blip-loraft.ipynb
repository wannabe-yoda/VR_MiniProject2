{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11811845,"sourceType":"datasetVersion","datasetId":7418663},{"sourceId":11811859,"sourceType":"datasetVersion","datasetId":7418674},{"sourceId":11842321,"sourceType":"datasetVersion","datasetId":7440409},{"sourceId":400019,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":327349,"modelId":348231}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('/kaggle/input/qna-final/qna_final.csv')\nprint(data.columns)\n\n# Split into train (80%) and val (20%) with shuffling and fixed random seed for reproducibility\ntrain_df, val_df = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n\nprint(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:58.682578Z","iopub.execute_input":"2025-05-18T04:00:58.682821Z","iopub.status.idle":"2025-05-18T04:01:01.623068Z","shell.execute_reply.started":"2025-05-18T04:00:58.682802Z","shell.execute_reply":"2025-05-18T04:01:01.622220Z"}},"outputs":[{"name":"stdout","text":"Index(['Image_ID', 'Item_ID', 'Question', 'Answer', 'Image_Path'], dtype='object')\nTrain size: 19109, Validation size: 4778\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nimport os\nimport string\n\n\n\nclass VQADataset(Dataset):\n    def __init__(self, data, image_dir, processor):\n        self.data = data\n        self.image_dir = image_dir\n        self.processor = processor\n    def normalize(self, text):\n        text = text.strip().lower()\n        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n        return text\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        question = self.normalize(item[\"Question\"])\n        answer = self.normalize(item[\"Answer\"])\n        image_path = os.path.join(self.image_dir, item[\"Image_Path\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Tokenize inputs\n        inputs = self.processor(image, question,padding=\"max_length\", \n                                max_length=64,\n                                 return_tensors=\"pt\")\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        inputs[\"labels\"] = self.processor.tokenizer(\n                            answer,\n                            padding=\"max_length\",       # pad answer to max length\n                            max_length=64,  # match input length or set your own max_length\n                            \n                            return_tensors=\"pt\"\n                        )[\"input_ids\"].squeeze(0)\n\n        return inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:01:01.623999Z","iopub.execute_input":"2025-05-18T04:01:01.624248Z","iopub.status.idle":"2025-05-18T04:01:05.921980Z","shell.execute_reply.started":"2025-05-18T04:01:01.624212Z","shell.execute_reply":"2025-05-18T04:01:05.921430Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import BlipProcessor\n\n# Load processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Initialize dataset\ntrain_dataset = VQADataset(\n    data = train_df, \n    image_dir=\"/kaggle/input/vqa-images\", \n    processor=processor\n)\nval_dataset = VQADataset(\n    data = val_df, \n    image_dir=\"/kaggle/input/vqa-images\", \n    processor=processor\n)\n# Test sample\nsample = train_dataset[0]\nprint({k: v.shape for k, v in sample.items()})\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\nval_loader = DataLoader(val_dataset, shuffle=True, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:01:05.923496Z","iopub.execute_input":"2025-05-18T04:01:05.923867Z","iopub.status.idle":"2025-05-18T04:01:25.593551Z","shell.execute_reply.started":"2025-05-18T04:01:05.923845Z","shell.execute_reply":"2025-05-18T04:01:25.592773Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 04:01:12.938986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747540873.134526      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747540873.190354      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d457934bf754abc8d746907effb3f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e569c3d92ff1477b95dbe174d39d88f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95c41b914264c1ca6b470f1ad74a13f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf966c221df4981bf16e7a75585162b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de44e44caa8a46188a121830c3d5438a"}},"metadata":{}},{"name":"stdout","text":"{'pixel_values': torch.Size([3, 384, 384]), 'input_ids': torch.Size([64]), 'attention_mask': torch.Size([64]), 'labels': torch.Size([64])}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch\n\nmodel_id = \"Salesforce/blip-vqa-base\"\nprocessor = BlipProcessor.from_pretrained(model_id)\nmodel = BlipForQuestionAnswering.from_pretrained(model_id)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,               # rank\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]  # common attention projection layers\n    #task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nfrom transformers import BlipProcessor\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\ntokenizer = processor.tokenizer\n\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:07:44.557953Z","iopub.execute_input":"2025-05-18T03:07:44.558686Z","iopub.status.idle":"2025-05-18T03:07:56.346250Z","shell.execute_reply.started":"2025-05-18T03:07:44.558658Z","shell.execute_reply":"2025-05-18T03:07:56.345231Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b79022e2424621965aa1cce522ce0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"975c101c47e2475aa2c55ce6019444da"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BlipForQuestionAnswering(\n      (vision_model): BlipVisionModel(\n        (embeddings): BlipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (encoder): BlipEncoder(\n          (layers): ModuleList(\n            (0-11): 12 x BlipEncoderLayer(\n              (self_attn): BlipAttention(\n                (dropout): Dropout(p=0.0, inplace=False)\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (projection): Linear(in_features=768, out_features=768, bias=True)\n              )\n              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (mlp): BlipMLP(\n                (activation_fn): GELUActivation()\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              )\n              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (text_encoder): BlipTextModel(\n        (embeddings): BlipTextEmbeddings(\n          (word_embeddings): Embedding(30524, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (encoder): BlipTextEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BlipTextLayer(\n              (attention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (crossattention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (intermediate): BlipTextIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BlipTextOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (text_decoder): BlipTextLMHeadModel(\n        (bert): BlipTextModel(\n          (embeddings): BlipTextEmbeddings(\n            (word_embeddings): Embedding(30524, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (encoder): BlipTextEncoder(\n            (layer): ModuleList(\n              (0-11): 12 x BlipTextLayer(\n                (attention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): Linear(in_features=768, out_features=768, bias=True)\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (crossattention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): Linear(in_features=768, out_features=768, bias=True)\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (intermediate): BlipTextIntermediate(\n                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BlipTextOutput(\n                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (cls): BlipTextOnlyMLMHead(\n          (predictions): BlipTextLMPredictionHead(\n            (transform): BlipTextPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (transform_act_fn): GELUActivation()\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (decoder): Linear(in_features=768, out_features=30524, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nimport random\nfrom tqdm import tqdm\n\n#train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n#val_loader = DataLoader(val_dataset, shuffle=True, batch_size=8)\nnum_epochs = 20\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nbest_val_acc = 0.0\n\ndef decode_predictions(generated_ids, tokenizer):\n    return [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n\ndef decode_labels(label_ids, tokenizer):\n    labels = label_ids.cpu().tolist()\n    return [tokenizer.decode(l, skip_special_tokens=True) for l in labels]\n\nfor epoch in range(num_epochs):\n    model.train()\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    # Training loop with tqdm\n    train_iter = tqdm(train_loader, desc=\"Training\", leave=False)\n    for batch_idx, batch in enumerate(train_iter):\n\n        input_ids = batch.pop(\"input_ids\").to(device)\n        pixel_values = batch.pop(\"pixel_values\").to(device)\n        attention_mask = batch.pop(\"attention_mask\").to(device)\n        labels = batch.pop(\"labels\").to(device)\n\n        outputs = model(input_ids=input_ids, pixel_values=pixel_values, \n                        attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        train_iter.set_postfix(loss=loss.item())\n        \"\"\"if batch_idx ==10:\n            break\"\"\"\n\n    model.eval()\n    total_correct = 0\n    total_count = 0\n    \n    with torch.no_grad():\n        val_iter = tqdm(val_loader, desc=\"Validation\", leave=False)\n        for batch_idx, batch in enumerate(val_iter):\n\n            input_ids = batch.pop(\"input_ids\").to(device)\n            pixel_values = batch.pop(\"pixel_values\").to(device)\n            attention_mask = batch.pop(\"attention_mask\").to(device)\n            labels = batch.pop(\"labels\").to(device)\n    \n            # ⬇️ Use generate instead of logits\n            generated_ids = model.generate(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask,\n                max_new_tokens=20  # Set as needed\n            )\n            \n            # Decode predictions\n            pred_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n            label_texts = decode_labels(labels, tokenizer)\n    \n            batch_correct = sum(p.strip().lower() == l.strip().lower()\n                                for p, l in zip(pred_texts, label_texts))\n            total_correct += batch_correct\n            total_count += len(pred_texts)\n    \n            val_iter.set_postfix(acc=total_correct / total_count)\n            \"\"\"if batch_idx == 10:\n                break\"\"\"\n\n\n    val_acc = total_correct / total_count\n    print(f\"Validation Accuracy: {val_acc:.4f}\")\n\n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_model.pt\")\n        print(\"Saved best model!\")\n\n    \"\"\"# Print 2 random train samples\n    train_samples = random.sample(list(train_dataset), 2)\n    print(\"\\nSample train predictions:\")\n    model.eval()\n    with torch.no_grad():\n        for sample in train_samples:\n            # Prepare inputs\n            inputs = {k: v.unsqueeze(0).to(device) for k, v in sample.items() if k != 'labels'}\n            labels = sample['labels'].unsqueeze(0).to(device)\n    \n            # Generate prediction\n            generated_ids = model.generate(**inputs, max_new_tokens=20)\n            pred_text = decode_predictions(generated_ids, tokenizer)[0]\n            label_text = decode_labels(labels, tokenizer)[0]\n    \n            print(f\"Predicted: {pred_text}\")\n            print(f\"Actual:    {label_text}\")\n    \n    # Print 2 random val samples\n    val_samples = random.sample(list(val_dataset), 2)\n    print(\"\\nSample val predictions:\")\n    with torch.no_grad():\n        for sample in val_samples:\n            inputs = {k: v.unsqueeze(0).to(device) for k, v in sample.items() if k != 'labels'}\n            labels = sample['labels'].unsqueeze(0).to(device)\n    \n            generated_ids = model.generate(**inputs, max_new_tokens=20)\n            pred_text = decode_predictions(generated_ids, tokenizer)[0]\n            label_text = decode_labels(labels, tokenizer)[0]\n    \n            print(f\"Predicted: {pred_text}\")\n            print(f\"Actual:    {label_text}\")\n    print(\"\\n\" + \"=\" * 40 + \"\\n\")\"\"\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:13:10.550360Z","iopub.execute_input":"2025-05-17T06:13:10.550914Z","execution_failed":"2025-05-17T15:09:52.780Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.5429\nSaved best model!\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4774\nEpoch 3/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4077\nEpoch 4/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4494\nEpoch 5/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.5795\nSaved best model!\nEpoch 6/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.5950\nSaved best model!\nEpoch 7/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6365\nSaved best model!\nEpoch 8/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6141\nEpoch 9/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6373\nSaved best model!\nEpoch 10/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6241\nEpoch 11/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6570\nSaved best model!\nEpoch 12/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6342\nEpoch 13/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6319\nEpoch 14/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.6666\nSaved best model!\nEpoch 15/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|▎         | 68/2389 [00:51<29:10,  1.33it/s, loss=8.33]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport random\nfrom tqdm import tqdm\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model.pt\"))\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\nval_loader   = DataLoader(val_dataset,   shuffle=False, batch_size=8)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer + Scheduler\noptimizer = AdamW(\n    model.parameters(),\n    lr=5e-5,\n    weight_decay=1e-2,           # 🔹 add weight decay\n)\ntotal_steps  = len(train_loader) * 20  # 20 epochs\nwarmup_steps = int(0.1 * total_steps)  # 10% warmup\nscheduler    = get_linear_schedule_with_warmup(\n    optimizer, warmup_steps, total_steps\n)\n\n# For mixed precision\nscaler = torch.cuda.amp.GradScaler()\n\n# Early stopping\nbest_val_acc = 0.0\npatience, wait = 3, 0\n\ndef decode_predictions(generated_ids, tokenizer):\n    return [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n\ndef decode_labels(label_ids, tokenizer):\n    labels = label_ids.cpu().tolist()\n    return [tokenizer.decode(l, skip_special_tokens=True) for l in labels]\n\nfor epoch in range(1, 21):\n    model.train()\n    print(f\"\\n→ Epoch {epoch}/20\")\n    train_iter = tqdm(train_loader, desc=\" Training\", leave=False)\n\n    for batch_idx, batch in enumerate(train_iter):\n        input_ids      = batch.pop(\"input_ids\").to(device)\n        pixel_values   = batch.pop(\"pixel_values\").to(device)\n        attention_mask = batch.pop(\"attention_mask\").to(device)\n        labels         = batch.pop(\"labels\").to(device)\n\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():  # 🔹 mixed precision\n            outputs = model(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            loss = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()  # 🔹 step LR every batch\n\n        train_iter.set_postfix(batch=batch_idx+1, loss=f\"{loss.item():.3f}\")\n\n    # —— Validation ——  \n    model.eval()\n    total_correct, total_count = 0, 0\n    val_iter = tqdm(val_loader, desc=\" Validation\", leave=False)\n\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(val_iter):\n            input_ids      = batch.pop(\"input_ids\").to(device)\n            pixel_values   = batch.pop(\"pixel_values\").to(device)\n            attention_mask = batch.pop(\"attention_mask\").to(device)\n            labels         = batch.pop(\"labels\").to(device)\n\n            # 🔹 Beam search decoding\n            generated_ids = model.generate(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask,\n                max_new_tokens=20,\n                num_beams=3,\n                early_stopping=True\n            )\n\n            pred_texts  = decode_predictions(generated_ids, tokenizer)\n            label_texts = decode_labels(labels, tokenizer)\n\n            for p, l in zip(pred_texts, label_texts):\n                if p.strip().lower() == l.strip().lower():\n                    total_correct += 1\n                total_count += 1\n\n            val_iter.set_postfix(acc=f\"{100*total_correct/total_count:.2f}%\")\n\n    val_acc = total_correct / total_count\n    print(f\"→ Validation Accuracy: {val_acc*100:.2f}%\")\n\n    # Early‑stop & checkpoint\n    if val_acc > best_val_acc:\n        best_val_acc, wait = val_acc, 0\n        torch.save(model.state_dict(), \"best_model.pt\")\n        print(\"✔️  Saved best model!\")\n    else:\n        wait += 1\n        if wait >= patience:\n            print(f\"⏹ Early stopping at epoch {epoch}\")\n            break\n\n# …after 20 epochs you can reload and continue training:\n# model.load_state_dict(torch.load(\"best_model.pt\"))\n# then run another loop (epochs 21–40) with the same setup.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T04:12:25.479512Z","iopub.execute_input":"2025-05-17T04:12:25.480123Z","iopub.status.idle":"2025-05-17T04:12:25.522612Z","shell.execute_reply.started":"2025-05-17T04:12:25.480099Z","shell.execute_reply":"2025-05-17T04:12:25.522020Z"}},"outputs":[{"name":"stdout","text":"Batch keys: dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'labels'])\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# install dependencies\n!pip install bert-score\n!git clone https://github.com/neulab/BARTScore.git\n# 1. Add BARTScore to path\nimport sys\nsys.path.append(\"./BARTScore\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:01:56.610302Z","iopub.execute_input":"2025-05-18T04:01:56.610893Z","iopub.status.idle":"2025-05-18T04:03:15.259682Z","shell.execute_reply.started":"2025-05-18T04:01:56.610865Z","shell.execute_reply":"2025-05-18T04:03:15.258944Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCloning into 'BARTScore'...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"remote: Enumerating objects: 220, done.\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 220 (delta 18), reused 14 (delta 14), pack-reused 194 (from 1)\u001b[K\nReceiving objects: 100% (220/220), 101.98 MiB | 23.74 MiB/s, done.\nResolving deltas: 100% (47/47), done.\nUpdating files: 100% (192/192), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import sys\n# 1. Add BARTScore to path\nsys.path.append(\"./BARTScore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:57:00.042190Z","iopub.execute_input":"2025-05-18T03:57:00.043048Z","iopub.status.idle":"2025-05-18T03:57:00.046501Z","shell.execute_reply.started":"2025-05-18T03:57:00.043014Z","shell.execute_reply":"2025-05-18T03:57:00.045919Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import sys, time, torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom bert_score import score as bert_score\nfrom peft import PeftModel\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import get_peft_model, LoraConfig\nfrom bart_score import BARTScorer  # assume path already added\nimport gc\n\n# 1. Device\n\n#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cpu\")\nprint(\"1 done\")\n\n# 2. Load processor & base model (full precision)\nmodel_id = \"Salesforce/blip-vqa-base\"\nprocessor = BlipProcessor.from_pretrained(model_id)\ntokenizer = processor.tokenizer\nprint(\"2 done\")\n\nmodel = BlipForQuestionAnswering.from_pretrained(model_id).to(device)\nprint(\"2 done\")\n\n# 3. Attach LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n)\nmodel = get_peft_model(model, lora_config)\nprint(\"3 done\")\n\n# 4. Load adapter weights\nadapter_state = torch.load(\"/kaggle/input/blip_best/transformers/default/1/best_model.pt\", map_location=\"cpu\")\nmodel.load_state_dict(adapter_state, strict=False)\nprint(\"4 done\")\n\n# 5. Build label ↔ ID mapping\nlabel_list = sorted(set(tokenizer.decode(sample[\"labels\"], skip_special_tokens=True).strip().lower()\n                        for sample in val_dataset))\nlabel2id = {l: i for i, l in enumerate(label_list)}\nid2label = {i: l for l, i in label2id.items()}\nprint(\"5 done\")\n\n# 6. Evaluation loop\nall_preds, all_labels = [], []\nall_pred_ids, all_true_ids = [], []\nprint(\"6 started\")\n\nt0 = time.time()\nmodel.eval()\nwith torch.no_grad():\n    print(\"6 loop started\")\n    for batch in tqdm(val_loader, desc=\"Evaluating\", mininterval=2.0):\n        input_ids = batch[\"input_ids\"].to(device)\n        pixel_values = batch[\"pixel_values\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"]\n\n        # Generate predictions\n        generated_ids = model.generate(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            max_new_tokens=20,\n            num_beams=3,\n            early_stopping=True\n        )\n\n        preds = [tokenizer.decode(g, skip_special_tokens=True).strip().lower()\n                 for g in generated_ids]\n        trues = [tokenizer.decode(l, skip_special_tokens=True).strip().lower()\n                 for l in labels]\n\n        all_preds.extend(preds)\n        all_labels.extend(trues)\n        all_pred_ids.extend([label2id.get(p, -1) for p in preds])\n        all_true_ids.extend([label2id.get(t, -1) for t in trues])\n\n        # Free memory\n        del input_ids, pixel_values, attention_mask, generated_ids\n        torch.cuda.empty_cache()\n\nt1 = time.time()\nprint(f\"\\n✅ Inference completed in {t1 - t0:.1f}s\")\n\n# 7. Classification Metrics\nmask = [(t >= 0 and p >= 0) for t, p in zip(all_true_ids, all_pred_ids)]\ny_true = [t for m, t in zip(mask, all_true_ids) if m]\ny_pred = [p for m, p in zip(mask, all_pred_ids) if m]\n\nacc = accuracy_score(y_true, y_pred)\nprec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\nrec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\nf1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n\nprint(\"\\n=== Classification Metrics ===\")\nprint(f\"Accuracy       : {acc:.4f}\")\nprint(f\"Precision (M)  : {prec:.4f}\")\nprint(f\"Recall    (M)  : {rec:.4f}\")\nprint(f\"F1 Score  (M)  : {f1:.4f}\")\n\n# 8. BERTScore (semantic similarity)\nprint(\"\\nComputing BERTScore...\")\nt0 = time.time()\np, r, f = bert_score(\n    all_preds, all_labels,\n    lang=\"en\", model_type=\"bert-base-uncased\",\n    rescale_with_baseline=True\n)\nt1 = time.time()\nprint(\"=== BERTScore ===\")\nprint(f\"P: {p.mean().item():.4f}  R: {r.mean().item():.4f}  F1: {f.mean().item():.4f}\")\nprint(f\"Computed in {t1 - t0:.1f}s\")\n\n# 9. BARTScore (semantic entailment) on CPU\nprint(\"\\nComputing BARTScore...\")\nt0 = time.time()\nbart_scorer = BARTScorer(device=\"cpu\", checkpoint=\"facebook/bart-large-cnn\")\nbart_scores = bart_scorer.score(all_preds, all_labels, batch_size=4)\nmean_bart = sum(bart_scores) / len(bart_scores)\nprint(\"=== BARTScore ===\")\nprint(f\"Mean score: {mean_bart:.4f}\")\nprint(f\"Computed in {time.time() - t0:.1f}s\")\n\n# 10. Cleanup\ndel model, processor, tokenizer, bart_scorer\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:03:41.842892Z","iopub.execute_input":"2025-05-18T04:03:41.843222Z","iopub.status.idle":"2025-05-18T05:50:30.312400Z","shell.execute_reply.started":"2025-05-18T04:03:41.843191Z","shell.execute_reply":"2025-05-18T05:50:30.311778Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b2fe39cfa474787a69bacce0ab4f4b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6caa82be2ae47249288d83001b7cfe8"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 598/598 [1:38:53<00:00,  9.92s/it]","output_type":"stream"},{"name":"stdout","text":"\n✅ Inference completed in 5933.5s\n\n=== Classification Metrics ===\nAccuracy       : 0.7564\nPrecision (M)  : 0.2813\nRecall    (M)  : 0.2880\nF1 Score  (M)  : 0.2546\n\nComputing BERTScore...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f8cabef4a14ac986a5f01775a31729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"483cc505ef364832a3544f3611984883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7343c1d62ac446a1bccf3cd577d22c5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b7d743aeb8b4124a0a50c9da07a0f29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c6746e0f4134df08bd90278a01454e3"}},"metadata":{}},{"name":"stdout","text":"=== BERTScore ===\nP: 0.8245  R: 0.8127  F1: 0.8170\nComputed in 12.2s\n\nComputing BARTScore...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0897e89605f14027b5bed8944180cd67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c74ce057720847fd8caa125c109a7b8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b0e677d19d4fdda7ff0984b155fac7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79ee0f550204ac39c00cda1ac55b8ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"859434d9b6434e5cb3d6b0addd17c227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8899915baec4d578db91d8de82ab84d"}},"metadata":{}},{"name":"stdout","text":"=== BARTScore ===\nMean score: -3.6180\nComputed in 378.8s\n","output_type":"stream"}],"execution_count":5}]}